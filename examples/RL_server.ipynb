{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_transformers import *\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tnrange as trange\n",
    "from torch import nn\n",
    "import random\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "import run_squad as rs\n",
    "# from utils_squad import (read_squad_examples, convert_examples_to_features,\n",
    "#                          RawResult, write_predictions,\n",
    "#                          RawResultExtended, write_predictions_extended)\n",
    "import utils_squad as us\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import math\n",
    "import collections\n",
    "from io import open\n",
    "\n",
    "from pytorch_transformers.tokenization_bert import BasicTokenizer, whitespace_tokenize\n",
    "\n",
    "# Required by XLNet evaluation method to compute optimal threshold (see write_predictions_extended() method)\n",
    "from utils_squad_evaluate import find_all_best_thresh_v2, make_qid_to_has_ans, get_raw_scores\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [(BertForQuestionAnswering,       BertTokenizer,      'bert-large-uncased')]\n",
    "# MODELS = [(BertForQuestionAnswering,       BertTokenizer,      'bert-large-uncased-whole-word-masking-finetuned-squad')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_class, tokenizer_class, pretrained_weights in MODELS:\n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "    model = model_class.from_pretrained(pretrained_weights)\n",
    "    config = BertConfig.from_pretrained(pretrained_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/home/ubuntu/rl_testing/pytorch-transformers/examples/rl_state_dict.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_optimizer(BertForQuestionAnswering):\n",
    "    def __init__(self, config):\n",
    "        super(RL_optimizer, self).__init__(config)\n",
    "        self.reward = 0\n",
    "        self.bertqa = BertForQuestionAnswering(config)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None,\n",
    "                end_positions=None, position_ids=None, head_mask=None):\n",
    "        \n",
    "        outputs = self.bertqa(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None,\n",
    "                end_positions=None, position_ids=None, head_mask=None)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = RL_optimizer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-4, eps=0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_rewards():\n",
    "#     \"\"\"For now, we're using random rewards to check if the flow works fine\"\"\"\n",
    "#     return random.choice([-0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args_list:\n",
    "    def __init__(self):\n",
    "        self.local_rank = -1\n",
    "        self.n_gpu = 1\n",
    "        self.train_file = '/home/ubuntu/question_generation/data/train-v2.0.json'\n",
    "        self.evaluate = 0\n",
    "        self.predict_file = '/home/ubuntu/question_generation/data/dev-v2.0.json'\n",
    "        self.eval_batch_size = 1\n",
    "        self.model_type = 'bert'\n",
    "        self.model_name_or_path = 'bert-base-uncased'\n",
    "        self.output_dir = './outputs/'\n",
    "        self.tokenizer_name = 'BertTokenizer'\n",
    "        self.max_seq_length = 384\n",
    "        self.version_2_with_negative = True\n",
    "        self.doc_stride = 128\n",
    "        self.max_query_length = 64\n",
    "        self.device = torch.device('cuda')\n",
    "        self.overwrite_cache = False\n",
    "        self.null_score_diff_threshold = 0.0\n",
    "        self.n_best_size = 20\n",
    "        self.max_answer_length = 30\n",
    "        self.verbose_logging = False\n",
    "        self.do_lower_case = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = args_list()\n",
    "args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.set_device(args.device)\n",
    "model.to(args.device)\n",
    "# rl.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "# torch.distributed.init_process_group(backend='nccl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Train\n",
    "# dataset = rs.load_and_cache_examples(args, tokenizer)\n",
    "dataset, examples, features = rs.load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=True, number_of_examples = 5)\n",
    "train_sampler = SequentialSampler(dataset)\n",
    "train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Eval\n",
    "dataset, examples, features = rs.load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n",
    "eval_sampler = SequentialSampler(dataset) if args.local_rank == -1 else DistributedSampler(dataset)\n",
    "eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    ## For Train\n",
    "    # dataset = rs.load_and_cache_examples(args, tokenizer)\n",
    "    dataset, examples, features = rs.load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=True, number_of_examples = 5)\n",
    "    train_sampler = SequentialSampler(dataset) if args.local_rank == -1 else DistributedSampler(dataset)\n",
    "    train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=2)\n",
    "    return dataset, examples, features, train_sampler, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, examples, features, train_sampler, train_dataloader = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_rewards(model, epochs = 2):\n",
    "    \n",
    "    train_iterator = trange(int(epochs), desc=\"Epoch\", disable=-1 not in [-1, 0])\n",
    "    model.zero_grad()\n",
    "    \n",
    "    for tr_iter in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=-1 not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            torch.cuda.empty_cache()\n",
    "            model.train()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "#             break\n",
    "            outputs = model(**form_inputs(batch))\n",
    "            loss = outputs[0]\n",
    "            \n",
    "            results, used_features = get_all_results(batch, outputs)\n",
    "            used_examples = get_required_examples(used_features, examples)\n",
    "            all_predictions = write_predictions(used_examples, used_features, results, args.n_best_size,\n",
    "                        args.max_answer_length, args.do_lower_case, output_prediction_file,\n",
    "                        output_nbest_file, output_null_log_odds_file, args.verbose_logging,\n",
    "                        args.version_2_with_negative, args.null_score_diff_threshold)\n",
    "            \n",
    "            rewards = calc_rewards(all_predictions, used_examples)\n",
    "            loss = loss + rewards\n",
    "            if step % 2000 == 0:\n",
    "                print('reward = ', rewards)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "        print(f'The loss in step ', tr_iter, ' is ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ca3ceb380546edb46f3635a203c452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08452d31586e447e8dd1cb22180a9662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=1256, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward =  2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/rl_testing/RL/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ubuntu/rl_testing/RL/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ubuntu/rl_testing/RL/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss in step  0  is  tensor(1.0073, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4483e82e13415eab90abd49a21b393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=1256, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward =  2.0\n",
      "The loss in step  1  is  tensor(1.0008, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9d15fad45d436998a991810e97b569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=1256, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward =  2.0\n",
      "The loss in step  2  is  tensor(1.0004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_with_rewards(model, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc457252b5245ba975b61bf156d12e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf193008515421998d8656964f7696b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=1256, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_iterator = trange(int(1), desc=\"Epoch\", disable=-1 not in [-1, 0])\n",
    "for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=-1 not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "#             print(batch)\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "#             print(batch)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'rl_state_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_inputs(batch):\n",
    "    inputs = {}\n",
    "    inputs = {'input_ids': batch[0],\n",
    "                      'attention_mask':  batch[1], \n",
    "                      'token_type_ids':  None if args.model_type == 'xlm' else batch[2],  \n",
    "                      'start_positions': batch[3], \n",
    "                      'end_positions':   batch[4]}\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs = model(**form_inputs(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8115, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor([[ -5.8113,  -9.5784, -10.2133, -10.1675, -10.2139, -10.2775, -10.6017,\n",
       "          -10.6953, -10.4109,  -4.7854,  -8.3866,  -9.7981,  -7.0030,  -9.3320,\n",
       "           -9.1561,  -9.5619,  -9.8994,  -9.6349,  -6.5670, -10.3656, -10.0256,\n",
       "           -6.6386,  -9.7728, -10.7530, -10.6535,  -8.6792, -10.4406,  -9.9447,\n",
       "          -10.0494, -10.2658,  -9.5349, -10.8343,  -8.2652,  -8.2829,  -5.7821,\n",
       "           -8.3685,  -8.4558,  -5.2630,  -9.5228, -10.0244, -10.7324, -10.1083,\n",
       "           -8.6699,  -9.9736,  -9.4496,  -9.7402,  -9.6530,  -9.9214, -10.3332,\n",
       "          -10.0907,  -9.9857,  -8.7563, -10.6464,  -9.7717,  -9.3555,  -7.7645,\n",
       "           -9.7250,  -7.8169,  -8.9943,  -9.7767,  -9.7294, -10.3602,  -8.8986,\n",
       "           -7.7574,  -9.9791,  -8.6837,  -9.3785,  -9.0260, -10.0453,  -9.3273,\n",
       "           -8.8221,  -7.6876,  -5.8835,  -7.3954,  -6.4988,   2.8644,   3.6163,\n",
       "            5.8858,   3.4183,  -7.1160,  -7.2743,  -8.2778, -10.4501,  -6.0732,\n",
       "           -6.4326,  -7.3888,  -9.0669, -10.2312,  -7.2219,  -3.9668, -10.7776,\n",
       "           -7.9416,  -7.2889,  -8.3703, -10.1914, -10.9305,  -9.5196, -10.3469,\n",
       "          -10.6792,  -9.6175,  -9.2071,  -8.7459, -10.0242,  -8.6874, -10.2962,\n",
       "           -9.8280, -10.5060, -10.0402,  -9.6150, -10.2961, -10.4353,  -9.2409,\n",
       "           -6.3015, -10.1807, -10.1464,  -9.2584, -10.3591, -10.5306,  -9.5758,\n",
       "          -10.1209,  -8.8130,  -9.9003,  -9.5437,  -8.5539,  -8.5622,  -9.9423,\n",
       "           -7.6901, -10.5955,  -9.6431,  -8.5506,  -9.1342,  -9.6388,  -6.7398,\n",
       "           -9.9938,  -8.0679,  -6.7768,  -1.5596,  -8.7110,  -8.5760,  -9.9088,\n",
       "           -9.6686, -10.2184,  -9.6079, -10.2381,  -9.8257, -10.3651,  -9.7511,\n",
       "          -10.6221,  -9.9380,  -9.0667,  -9.4134, -10.0915, -10.4661, -10.1644,\n",
       "          -10.2613,  -9.9798, -10.3265,  -9.9658,  -9.6709,  -6.3468, -10.0873,\n",
       "          -10.0744,  -9.3772,  -8.8593, -10.3656, -10.7792, -10.2330, -10.7332,\n",
       "          -10.1534,  -9.4180,  -9.4459, -10.3314, -10.1974, -10.3280, -10.6505,\n",
       "          -10.8469, -10.6482, -10.8076, -10.8705, -10.6942, -10.7808, -10.3165,\n",
       "          -10.3325, -10.4398, -10.6017, -10.8117, -10.6682, -10.7322, -10.0499,\n",
       "          -10.4358, -10.6452, -10.4128, -10.6129, -11.0349, -10.5519, -10.4370,\n",
       "          -10.3867, -10.8292, -10.5112, -10.5900, -10.7220, -10.5406, -10.5677,\n",
       "          -10.0635, -10.1791, -10.3870, -10.6567, -10.5924, -10.3384, -10.5618,\n",
       "          -10.6686, -10.3997, -10.5008, -10.4008, -10.8929, -10.7956, -10.7882,\n",
       "          -10.3211, -10.4987, -10.5281, -10.7601, -10.5599, -10.1056, -10.6262,\n",
       "          -10.6531, -10.4668, -10.6531, -10.8991, -10.2490, -10.4112, -10.2625,\n",
       "          -10.3588, -10.1396, -10.5804, -10.7745, -10.7794, -10.5138, -10.4556,\n",
       "          -10.4939, -10.3779, -10.4505, -10.8589, -10.3412, -10.9405, -10.4612,\n",
       "          -10.6999, -10.5489, -10.4988, -10.6309, -10.4058, -10.4894, -10.5993,\n",
       "          -10.5958, -10.6523, -10.4811, -10.5595, -10.5191, -10.1718, -10.5653,\n",
       "          -10.5818, -10.5442, -10.5811, -10.8225, -10.5864, -10.6118, -10.6312,\n",
       "          -10.8238, -10.5778, -10.8079, -10.4381, -10.6371, -10.5486, -10.2899,\n",
       "          -10.6753, -10.6224, -10.3790, -10.4901, -10.6944, -10.2592, -10.5193,\n",
       "          -10.5170, -10.5069, -10.7961, -10.7813, -10.4730, -10.0426, -10.3816,\n",
       "          -10.5402, -10.7059, -10.4090, -10.6505, -10.4567, -10.9424, -10.3040,\n",
       "          -10.1936, -10.5128, -10.7732, -10.6538, -10.5580, -10.6075,  -9.9187,\n",
       "          -10.0353, -10.6641, -10.7460, -10.6197, -10.4888, -10.6498, -10.8235,\n",
       "          -10.4587, -10.4934, -10.5843, -10.7288, -10.5500, -10.7445, -10.6635,\n",
       "          -10.6892, -10.6218, -10.5504, -10.3427, -10.5830, -10.5691, -10.4687,\n",
       "          -10.7047, -10.4511, -10.1386, -10.3535, -10.2017, -10.5346, -10.5885,\n",
       "          -10.6197, -10.4512, -10.6188, -10.8121, -10.7308, -10.7357, -10.6799,\n",
       "          -10.3825, -10.4432, -10.8478, -10.4680, -10.5966, -10.4836, -10.4574,\n",
       "          -10.6476, -10.5814, -10.8229, -10.3229, -10.3649, -10.3714, -10.5345,\n",
       "          -10.3523, -10.4508, -10.3797, -10.5783, -10.6976, -10.6591, -10.6690,\n",
       "          -10.7142, -10.7398, -10.7243, -10.8419, -10.5127, -10.2455, -10.5776,\n",
       "          -10.6111, -10.6972, -10.4336, -10.6714, -10.8887, -10.3724, -10.4888,\n",
       "          -10.4867, -10.3209, -10.8029, -10.8104, -10.6580, -10.6961, -10.5074,\n",
       "          -10.4089, -10.6552, -10.3555, -10.4439, -10.4481, -10.7690],\n",
       "         [ -8.6201,  -9.1977,  -9.0478, -10.3583,  -9.9562,  -9.9345,  -9.3277,\n",
       "          -10.2000, -10.5679, -10.5207, -10.2526, -10.6264, -10.5326, -10.5875,\n",
       "           -6.6145,  -9.2533,  -9.8924,  -8.8406, -10.0608,  -8.5333,  -9.5512,\n",
       "          -10.1486, -10.0224,  -6.3525, -10.4049,  -9.6461,  -8.5466,  -9.6293,\n",
       "          -10.6881, -10.0258,  -8.7691, -10.3487,  -9.5621,  -9.4180, -10.0166,\n",
       "           -9.4788,  -9.9096,  -9.4863,  -9.7811,  -7.8601,  -9.5436,  -9.5259,\n",
       "           -8.3297,  -9.8418, -10.9501, -10.2375, -10.2460,  -8.3565,  -9.7770,\n",
       "           -8.8657,  -9.5957,  -9.0785,  -8.8837, -10.8580,  -9.1114,  -9.3920,\n",
       "           -9.2843, -10.8305,  -9.9750,  -9.8642,  -7.5117,  -9.5500,  -8.8304,\n",
       "           -7.5833,  -7.1344,  -7.8823,  -6.6449,  -0.5276,   7.4734,  -6.9865,\n",
       "           -0.4079,  -4.6634,  -9.5417, -10.5864,  -9.4525,  -8.6637, -10.3633,\n",
       "          -10.1283, -10.2031,  -9.3273,  -9.1276,  -8.4785,  -8.8420,  -9.0194,\n",
       "          -10.0280,  -9.0821,  -8.9151, -10.3732,  -7.6115,  -6.5909,  -9.5171,\n",
       "           -9.0572,  -8.9277,  -9.2403,  -7.5497, -10.8602,  -9.5015,  -7.8443,\n",
       "          -10.4596, -10.6302, -11.0569, -10.4435,  -9.9202, -10.6877,  -9.8682,\n",
       "           -9.5591,  -9.9143, -10.3480, -10.4380,  -9.9305,  -9.3582, -10.9181,\n",
       "          -10.2028,  -9.4296, -10.7995,  -9.8246,  -9.6445,  -6.4459,  -9.7584,\n",
       "           -9.4998,  -9.5032, -10.3349, -10.2459,  -9.5773,  -9.6187, -10.2022,\n",
       "           -9.8219,  -9.5750,  -9.7422,  -9.2608, -10.2161, -10.0668, -10.6188,\n",
       "          -10.8226,  -8.9714,  -9.8262,  -9.7179,  -7.9633,  -9.8909,  -9.1774,\n",
       "           -8.1340,  -8.1349,  -9.5153,  -9.4033,  -9.8019,  -9.8948, -10.4448,\n",
       "          -10.4652,  -9.8851,  -9.2115,  -9.9544,  -9.6140, -10.4799, -10.0545,\n",
       "           -9.4570,  -9.4925,  -9.8462, -10.5875, -10.3335, -10.0352,  -9.7393,\n",
       "          -10.2347,  -9.5197,  -9.8973,  -6.6728,  -9.9824,  -9.8951,  -9.8234,\n",
       "           -8.3339, -10.0758,  -9.4670,  -9.8175,  -9.6640,  -9.3649,  -9.4068,\n",
       "           -9.4640,  -9.1513,  -9.9426, -10.6988, -10.5789, -10.8561, -11.1038,\n",
       "          -10.9659, -10.8974, -10.8953, -11.1167, -10.8340, -10.8376, -11.1329,\n",
       "          -11.0444, -10.7858, -11.1399, -10.7973, -11.1071, -11.0678, -11.0310,\n",
       "          -10.3817, -10.8651, -11.0526, -11.1772, -10.8323, -10.6088, -10.8012,\n",
       "          -10.8075, -10.9229, -10.6540, -10.4256, -10.9175, -10.8503, -10.7535,\n",
       "          -10.7760, -11.1945, -10.9969, -10.8953, -11.1909, -10.9656, -10.7026,\n",
       "          -11.1346, -10.7019, -11.0520, -11.2011, -10.9336, -10.9773, -11.2566,\n",
       "          -10.8780, -10.7549, -10.6976, -10.8937, -10.9744, -10.8002, -10.9833,\n",
       "          -10.6387, -10.7695, -10.7644, -10.9730, -10.7405, -10.7983, -10.8896,\n",
       "          -11.3313, -10.8334, -10.8343, -10.7616, -10.9671, -10.5380, -10.9945,\n",
       "          -10.8093, -10.8751, -10.9761, -10.8711, -10.8905, -11.0105, -10.9722,\n",
       "          -10.4754, -10.9969, -10.7029, -10.7527, -10.6914, -11.0479, -10.8198,\n",
       "          -10.5776, -11.0920, -10.8978, -11.0702, -10.9633, -11.0156, -10.9083,\n",
       "          -10.9172, -10.8922, -10.9846, -10.8144, -10.7736, -10.8002, -11.0513,\n",
       "          -10.8253, -10.8057, -10.9270, -10.9393, -10.9285, -10.9009, -10.9397,\n",
       "          -10.8219, -10.9737, -10.7278, -10.9055, -10.7222, -11.0765, -10.9145,\n",
       "          -11.0656, -10.5953, -10.7030, -10.6074, -10.9047, -10.7571, -10.8826,\n",
       "          -11.1309, -10.8380, -10.8847, -10.7874, -11.1532, -10.8596, -11.2199,\n",
       "          -10.7799, -10.7166, -10.9306, -10.5895, -10.5878, -10.9022, -10.9019,\n",
       "          -10.8853, -10.9645, -10.7874, -10.9958, -10.9484, -10.8349, -10.8916,\n",
       "          -10.8914, -10.9250, -10.7912, -10.9224, -10.8921, -10.6429, -10.8347,\n",
       "          -10.5544, -10.7378, -10.7887, -10.4194, -10.8369, -10.4278, -10.5834,\n",
       "          -10.9865, -10.9130, -10.7590, -11.1322, -11.1831, -10.9101, -10.8551,\n",
       "          -10.9859, -10.7535, -10.6571, -10.7433, -10.9654, -10.7227, -10.7939,\n",
       "          -10.7604, -10.7174, -10.6681, -10.5350, -11.0081, -10.7582, -10.5363,\n",
       "          -10.9617, -10.7468, -10.8428, -11.0922, -10.9670, -10.7846, -11.1800,\n",
       "          -10.7764, -10.5650, -10.6174, -10.6201, -10.8722, -10.9203, -10.6943,\n",
       "          -10.9131, -10.8208, -11.0254, -10.5880, -11.0020, -11.0270, -10.9613,\n",
       "          -11.0442, -10.8212, -10.9515, -11.0944, -11.0858, -10.7553, -10.6416,\n",
       "          -10.8004, -10.7843, -10.6050, -11.0669, -11.2758, -11.0415]],\n",
       "        device='cuda:0', grad_fn=<SqueezeBackward1>),\n",
       " tensor([[ -5.1433,  -9.8243,  -9.8376,  -9.4416,  -9.9594, -10.3702, -10.0598,\n",
       "           -9.9330,  -9.7154,  -5.8870,  -9.0453,  -8.6229,  -8.1277,  -9.0528,\n",
       "           -7.4184,  -9.0348,  -9.9474,  -9.1877,  -7.5546, -10.0098,  -9.9215,\n",
       "           -6.9892,  -9.1663,  -9.6217,  -9.1316,  -8.9646,  -9.8016,  -9.4803,\n",
       "           -8.7913,  -9.7753,  -8.3383,  -9.2468,  -9.4867,  -8.0296,  -6.2345,\n",
       "           -7.0655,  -6.0812,  -4.8435,  -7.0409,  -9.3732,  -9.6724,  -9.5261,\n",
       "           -8.9391,  -9.4094,  -8.6028,  -8.1520,  -9.8291,  -9.6505,  -9.0843,\n",
       "           -9.5103,  -8.7621,  -8.4960,  -9.7523,  -9.5660,  -9.6999,  -8.1206,\n",
       "           -7.3353,  -6.7866,  -5.6609,  -9.9075,  -9.9384, -10.0456,  -9.8324,\n",
       "           -8.6098, -10.0588,  -8.3898,  -7.0642,  -8.9099,  -9.6263,  -6.8338,\n",
       "           -6.6531, -10.4916,  -9.8944,  -9.5286,  -8.3743,  -6.0979,  -5.6557,\n",
       "           -3.1446,   6.1347,  -7.8518,  -9.3401,  -6.5689, -10.4427,  -8.8115,\n",
       "           -6.9253,  -6.9661,  -8.1387, -10.0010,  -7.3551,  -5.8918,  -9.8750,\n",
       "           -7.3651,  -2.7698,  -4.2780,  -9.9277,  -9.4449,  -9.5471,  -9.6974,\n",
       "           -9.3989, -10.0835,  -9.0958,  -5.7286, -10.2854,  -8.6354, -10.1328,\n",
       "           -9.2980, -10.1303, -10.5506,  -9.5391, -10.5600,  -9.0480,  -9.8138,\n",
       "           -6.9381,  -9.6340,  -9.2825,  -8.0879,  -9.9716,  -9.4010,  -7.0233,\n",
       "           -7.9180,  -9.5648,  -9.5161,  -8.9653,  -9.3066,  -9.1491,  -9.8384,\n",
       "           -8.0622, -10.2314,  -7.6283,  -9.8528,  -8.8650,  -7.9498,  -7.7269,\n",
       "           -9.3447,  -5.5330,  -9.4862,  -1.3616,  -5.1508,  -6.9117,  -9.3483,\n",
       "           -9.3618,  -9.0028,  -9.3203,  -9.5907,  -9.9380,  -9.4752,  -8.9718,\n",
       "           -8.5617,  -9.4286,  -8.9505,  -8.7554,  -7.9694,  -8.9036,  -9.4479,\n",
       "           -9.4685,  -8.9121,  -9.6397,  -8.3017,  -8.9729,  -6.9701,  -8.4120,\n",
       "           -8.6305,  -9.1090,  -9.7139,  -9.8056,  -8.6147,  -9.0309,  -9.4386,\n",
       "           -9.1457,  -8.9859,  -7.7233,  -7.9492,  -8.8419,  -9.9161, -10.2988,\n",
       "          -10.0414, -10.2641, -10.1670, -10.2133, -10.3902, -10.2628, -10.4365,\n",
       "          -10.4806, -10.3156, -10.1368, -10.2559, -10.2333, -10.1838, -10.2373,\n",
       "          -10.3852,  -9.9564, -10.2997, -10.2154,  -9.9141, -10.3441, -10.4376,\n",
       "          -10.4252, -10.3045, -10.5775, -10.0010, -10.2422, -10.4734, -10.1792,\n",
       "          -10.5312, -10.5765, -10.1744, -10.1135, -10.3246, -10.3846, -10.4300,\n",
       "          -10.1514, -10.3918, -10.5090, -10.3927, -10.1049, -10.2187, -10.2056,\n",
       "          -10.2759, -10.4124, -10.4372, -10.2112, -10.3778, -10.4222, -10.2602,\n",
       "          -10.0652, -10.5222, -10.2123,  -9.9202, -10.3268, -10.3190, -10.3425,\n",
       "          -10.1825, -10.4362, -10.0882, -10.1154, -10.3302, -10.4810, -10.3738,\n",
       "          -10.3958, -10.4465, -10.2709,  -9.9524, -10.3982, -10.0350, -10.6033,\n",
       "          -10.5191, -10.4094, -10.4244, -10.2646, -10.5105, -10.4245, -10.2784,\n",
       "           -9.9867, -10.3024, -10.5275, -10.1793, -10.4503, -10.4245, -10.3109,\n",
       "          -10.2756, -10.3871, -10.1876, -10.2700, -10.3544, -10.3774, -10.1842,\n",
       "          -10.2009, -10.3751, -10.1227, -10.4322, -10.3830, -10.2892, -10.4270,\n",
       "          -10.0557, -10.4537, -10.3942, -10.0117, -10.2081, -10.7360, -10.1110,\n",
       "          -10.3294, -10.5235, -10.1622, -10.2325, -10.3256, -10.2834, -10.4685,\n",
       "          -10.3545, -10.1300, -10.4879, -10.3068, -10.4061, -10.1852, -10.1692,\n",
       "          -10.3733, -10.1507, -10.1535, -10.1035, -10.3825, -10.4594, -10.3622,\n",
       "          -10.4541, -10.2278, -10.3497, -10.3277, -10.3777, -10.1907, -10.2016,\n",
       "          -10.1898, -10.3584, -10.2484, -10.1061, -10.2292,  -9.8406, -10.3509,\n",
       "          -10.0899, -10.1831, -10.2565, -10.4703, -10.4998, -10.3439, -10.5988,\n",
       "          -10.2304, -10.6246, -10.2252, -10.3564, -10.2751, -10.1891, -10.4268,\n",
       "          -10.0947, -10.5402, -10.3448, -10.2348, -10.0914, -10.3464, -10.2110,\n",
       "          -10.1862, -10.1760, -10.3413, -10.2631, -10.3274, -10.3590, -10.4094,\n",
       "          -10.2663, -10.3339, -10.0580, -10.4225, -10.4653, -10.3463, -10.3921,\n",
       "          -10.5387, -10.4766, -10.2268, -10.1096, -10.2322, -10.5918, -10.0215,\n",
       "          -10.0452, -10.2840, -10.3039, -10.1859, -10.2858, -10.3268, -10.2979,\n",
       "          -10.3692, -10.0079, -10.6847, -10.4131, -10.2467, -10.7643, -10.4754,\n",
       "          -10.5222, -10.7403, -10.1158, -10.2580, -10.2184, -10.2218, -10.3173,\n",
       "          -10.3898, -10.3041, -10.2691, -10.2343, -10.3231, -10.2008],\n",
       "         [ -5.9727,  -8.8059,  -7.3874,  -9.3530,  -9.4929,  -9.4297,  -8.1227,\n",
       "          -10.0730, -10.1094,  -9.7305,  -9.8896,  -9.6493, -10.0608,  -9.6107,\n",
       "           -6.9618,  -8.2956,  -9.2045,  -8.8008,  -9.2219,  -6.7088,  -8.9588,\n",
       "           -8.9750,  -8.9019,  -6.6676,  -9.7231,  -9.0931,  -8.6101,  -8.7635,\n",
       "           -8.9826,  -8.0332,  -8.0677,  -9.1980,  -8.5571,  -8.0787,  -8.4120,\n",
       "           -7.2953,  -8.1366,  -8.9466,  -8.6693,  -7.7463,  -8.7588,  -6.5368,\n",
       "           -6.8580,  -7.2523,  -9.0145,  -9.1222,  -9.2133,  -8.5727,  -7.2401,\n",
       "           -8.3048,  -7.8336,  -8.7146,  -8.1448,  -9.3078,  -8.4083,  -6.7473,\n",
       "           -9.0598,  -9.1940,  -9.4617,  -9.0404,  -7.2381,  -7.8374,  -7.3457,\n",
       "           -5.5135,  -7.3822,  -8.3312,  -7.1323,  -4.6548,  -2.3000,  -7.4187,\n",
       "            7.9746,   3.3288,  -9.0694,  -9.9043,  -7.3095,  -5.4774,  -8.8830,\n",
       "           -9.8408,  -9.7014,  -8.7285,  -8.9051,  -8.7884,  -8.6993,  -7.6996,\n",
       "           -9.5443,  -8.9729,  -7.7122,  -9.0972,  -7.7203,  -7.1439,  -8.5279,\n",
       "           -8.0008,  -8.8734,  -8.3228,  -7.2825,  -9.3973,  -7.8919,  -5.7322,\n",
       "           -7.5522,  -9.4457,  -9.1977,  -9.3655,  -8.4105,  -8.8635,  -8.8479,\n",
       "           -8.3781,  -8.0413,  -8.6675,  -9.2360,  -8.7361,  -8.9711,  -9.2562,\n",
       "           -9.7092,  -8.7262, -10.0807,  -8.6948,  -9.5615,  -6.7663,  -8.4285,\n",
       "           -9.2409,  -8.0014,  -8.7561,  -8.4815,  -7.7248,  -7.4458,  -9.3689,\n",
       "           -8.6331,  -8.5993,  -8.8752,  -8.5686,  -9.0704,  -9.7525,  -9.8284,\n",
       "           -9.1206,  -8.7769,  -8.7965,  -8.0064,  -7.6522,  -8.6808,  -7.3940,\n",
       "           -8.4540,  -7.2523,  -7.8131,  -6.6484,  -8.0659,  -8.8181,  -9.1727,\n",
       "           -9.1118,  -8.8487,  -9.0898,  -8.3869,  -8.2879,  -8.4978,  -9.1625,\n",
       "           -8.3715,  -8.4309,  -7.9207,  -9.5778,  -9.3135,  -8.8760,  -8.6315,\n",
       "           -9.1128,  -8.3151,  -9.2151,  -6.2852,  -8.4480,  -8.6395,  -9.3563,\n",
       "           -7.8117,  -9.1801,  -8.2034,  -8.9653,  -8.5197,  -9.1050,  -8.8677,\n",
       "           -7.9522,  -7.8011,  -8.2626,  -9.0571,  -9.4326,  -9.9093,  -9.6720,\n",
       "           -9.9170,  -9.5380,  -9.6728,  -9.3987,  -9.6866,  -9.6445,  -9.5704,\n",
       "           -9.5937,  -9.6053,  -9.6348,  -9.6735,  -9.5131,  -9.5218,  -9.7220,\n",
       "           -9.7045,  -9.7409,  -9.4625,  -9.7195,  -9.7934,  -9.7972,  -9.7026,\n",
       "           -9.6242,  -9.4265,  -9.5083,  -9.5383,  -9.8430, -10.0403,  -9.5858,\n",
       "           -9.5781,  -9.6155,  -9.3487,  -9.8567,  -9.5854,  -9.7042,  -9.7474,\n",
       "           -9.4636,  -9.8908,  -9.6761,  -9.5020,  -9.7206,  -9.6077,  -9.7794,\n",
       "           -9.4850,  -9.6928,  -9.5656,  -9.6152,  -9.6459, -10.0116,  -9.4676,\n",
       "           -9.6673,  -9.6596,  -8.9706,  -9.5240,  -9.8502,  -9.8142,  -9.4668,\n",
       "           -9.4387,  -9.6125,  -9.8676, -10.0192,  -9.9621,  -9.7295,  -9.5490,\n",
       "           -9.9771,  -9.8780,  -9.5349,  -9.5417,  -9.7387,  -9.3922,  -9.5557,\n",
       "           -9.7293,  -9.4584,  -9.7081,  -9.5634,  -9.9126,  -9.7027, -10.0184,\n",
       "           -9.3646,  -9.4653,  -9.6804,  -9.6148,  -9.6136,  -9.7910,  -9.4922,\n",
       "           -9.6765,  -9.5190,  -9.6330,  -9.5091,  -9.8623,  -9.9245,  -9.6692,\n",
       "           -9.5711,  -9.7488,  -9.5994,  -9.7939,  -9.9683,  -9.6119,  -9.8604,\n",
       "           -9.5566,  -9.6801, -10.0245,  -9.4827, -10.0795,  -9.4372,  -9.6688,\n",
       "           -9.5434,  -9.8277,  -9.7015,  -9.9011,  -9.6625,  -9.7758,  -9.4167,\n",
       "           -9.4092,  -9.6881,  -9.7293,  -9.6096,  -9.4555,  -9.7008,  -9.4014,\n",
       "           -9.7518,  -9.9418,  -9.3968,  -9.6611,  -9.7169,  -9.8062, -10.0026,\n",
       "           -9.9172,  -9.6988,  -9.3747,  -9.4332,  -9.7200,  -9.7465,  -9.8846,\n",
       "           -9.7552,  -9.6511,  -9.5687,  -9.5131,  -9.5498,  -9.8333,  -9.9486,\n",
       "           -9.6777,  -9.5624,  -9.7757,  -9.8382,  -9.2775,  -9.0107,  -9.5514,\n",
       "           -9.6514,  -9.4024,  -9.9256,  -9.4526,  -9.7534,  -9.9646,  -9.5748,\n",
       "           -9.7816, -10.1033, -10.0906,  -9.6166,  -9.7881,  -9.6741,  -9.6706,\n",
       "           -9.6818,  -9.6474,  -9.8709, -10.0966,  -9.5588,  -9.9443,  -9.4887,\n",
       "           -9.5586,  -9.6266,  -9.5139,  -9.6009,  -9.8454,  -9.4688,  -9.6307,\n",
       "           -9.7042,  -9.9635,  -9.8089,  -9.9841,  -9.6540,  -9.3204,  -9.9913,\n",
       "           -9.9148,  -9.7239,  -9.8646,  -9.5620,  -9.4120,  -9.7954,  -9.9010,\n",
       "           -9.7516,  -9.7282,  -9.2112,  -9.8311,  -9.7816,  -9.8920,  -9.9792,\n",
       "           -9.7556,  -9.6877,  -9.9730,  -9.0633,  -9.2958,  -9.7262]],\n",
       "        device='cuda:0', grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_results(batch, outputs):\n",
    "    example_indices = batch[3]\n",
    "    all_results = []\n",
    "    used_features = []\n",
    "    for i, example_index in enumerate(example_indices):\n",
    "#                 print(example_index)\n",
    "                eval_feature = features[example_index.item()]\n",
    "                used_features.append(eval_feature)\n",
    "                unique_id = int(eval_feature.unique_id)\n",
    "                if args.model_type in ['xlnet', 'xlm']:\n",
    "                    # XLNet uses a more complex post-processing procedure\n",
    "                    result = us.RawResultExtended(unique_id            = unique_id,\n",
    "                                               start_top_log_probs  = outputs[0][i].tolist(),\n",
    "                                               start_top_index      = (outputs[1][i]).tolist(),\n",
    "                                               end_top_log_probs    = (outputs[2][i]).tolist(),\n",
    "                                               end_top_index        = (outputs[3][i]).tolist(),\n",
    "                                               cls_logits           = (outputs[4][i]).tolist())\n",
    "                else:\n",
    "                    result = us.RawResult(unique_id    = unique_id,\n",
    "                                       start_logits = outputs[1][i].tolist(),\n",
    "                                       end_logits   = outputs[2][i].tolist())\n",
    "#                     print(unique_id)\n",
    "                all_results.append(result)\n",
    "    return all_results, used_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, used_features = get_all_results(batch, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_required_examples(used_features, all_examples):\n",
    "    used_examples = []\n",
    "    for f in used_features:\n",
    "        used_examples.append(all_examples[f.example_index])\n",
    "    return used_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_examples = get_required_examples(used_features, examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "prefix = ''\n",
    "output_prediction_file = os.path.join(args.output_dir, \"predictions_{}.json\".format(prefix))\n",
    "output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions_{}.json\".format(prefix))\n",
    "if args.version_2_with_negative:\n",
    "        output_null_log_odds_file = os.path.join(args.output_dir, \"null_odds_{}.json\".format(prefix))\n",
    "else:\n",
    "        output_null_log_odds_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'write_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7d6c9ac849c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m all_predictions = write_predictions(used_examples, used_features, results, args.n_best_size,\n\u001b[0m\u001b[1;32m      2\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_answer_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_prediction_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0moutput_nbest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_null_log_odds_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_logging\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         args.version_2_with_negative, args.null_score_diff_threshold)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'write_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "all_predictions = write_predictions(used_examples, used_features, results, args.n_best_size,\n",
    "                        args.max_answer_length, args.do_lower_case, output_prediction_file,\n",
    "                        output_nbest_file, output_null_log_odds_file, args.verbose_logging,\n",
    "                        args.version_2_with_negative, args.null_score_diff_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(all_examples, all_features, all_results, n_best_size,\n",
    "                      max_answer_length, do_lower_case, output_prediction_file,\n",
    "                      output_nbest_file, output_null_log_odds_file, verbose_logging,\n",
    "                      version_2_with_negative, null_score_diff_threshold):\n",
    "    \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n",
    "    logger.info(\"Writing predictions to: %s\" % (output_prediction_file))\n",
    "    logger.info(\"Writing nbest to: %s\" % (output_nbest_file))\n",
    "\n",
    "    example_index_to_features = collections.defaultdict(list)\n",
    "    count_f = 0\n",
    "    for feature in all_features:\n",
    "        example_index_to_features[count_f].append(feature)\n",
    "        count_f += 1\n",
    "\n",
    "    unique_id_to_result = {}\n",
    "    for result in all_results:\n",
    "        unique_id_to_result[result.unique_id] = result\n",
    "\n",
    "    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"PrelimPrediction\",\n",
    "        [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
    "\n",
    "    all_predictions = collections.OrderedDict()\n",
    "    all_nbest_json = collections.OrderedDict()\n",
    "    scores_diff_json = collections.OrderedDict()\n",
    "\n",
    "    for (example_index, example) in enumerate(all_examples):\n",
    "        features = example_index_to_features[example_index]\n",
    "\n",
    "        prelim_predictions = []\n",
    "        # keep track of the minimum score of null start+end of position 0\n",
    "        score_null = 1000000  # large and positive\n",
    "        min_null_feature_index = 0  # the paragraph slice with min null score\n",
    "        null_start_logit = 0  # the start logit at the slice with min null score\n",
    "        null_end_logit = 0  # the end logit at the slice with min null score\n",
    "        for (feature_index, feature) in enumerate(features):\n",
    "            \n",
    "#             if feature.unique_id == 1000000075:\n",
    "                \n",
    "            result = unique_id_to_result[feature.unique_id]\n",
    "            start_indexes = us._get_best_indexes(result.start_logits, n_best_size)\n",
    "            end_indexes = us._get_best_indexes(result.end_logits, n_best_size)\n",
    "            # if we could have irrelevant answers, get the min score of irrelevant\n",
    "            if version_2_with_negative:\n",
    "                feature_null_score = result.start_logits[0] + result.end_logits[0]\n",
    "                if feature_null_score < score_null:\n",
    "                    score_null = feature_null_score\n",
    "                    min_null_feature_index = feature_index\n",
    "                    null_start_logit = result.start_logits[0]\n",
    "                    null_end_logit = result.end_logits[0]\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # We could hypothetically create invalid predictions, e.g., predict\n",
    "                    # that the start of the span is in the question. We throw out all\n",
    "                    # invalid predictions.\n",
    "                    if start_index >= len(feature.tokens):\n",
    "                        continue\n",
    "                    if end_index >= len(feature.tokens):\n",
    "                        continue\n",
    "                    if start_index not in feature.token_to_orig_map:\n",
    "                        continue\n",
    "                    if end_index not in feature.token_to_orig_map:\n",
    "                        continue\n",
    "                    if not feature.token_is_max_context.get(start_index, False):\n",
    "                        continue\n",
    "                    if end_index < start_index:\n",
    "                        continue\n",
    "                    length = end_index - start_index + 1\n",
    "                    if length > max_answer_length:\n",
    "                        continue\n",
    "                    prelim_predictions.append(\n",
    "                        _PrelimPrediction(\n",
    "                            feature_index=feature_index,\n",
    "                            start_index=start_index,\n",
    "                            end_index=end_index,\n",
    "                            start_logit=result.start_logits[start_index],\n",
    "                            end_logit=result.end_logits[end_index]))\n",
    "        if version_2_with_negative:\n",
    "            prelim_predictions.append(\n",
    "                _PrelimPrediction(\n",
    "                    feature_index=min_null_feature_index,\n",
    "                    start_index=0,\n",
    "                    end_index=0,\n",
    "                    start_logit=null_start_logit,\n",
    "                    end_logit=null_end_logit))\n",
    "        prelim_predictions = sorted(\n",
    "            prelim_predictions,\n",
    "            key=lambda x: (x.start_logit + x.end_logit),\n",
    "            reverse=True)\n",
    "\n",
    "        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n",
    "\n",
    "        seen_predictions = {}\n",
    "        nbest = []\n",
    "        for pred in prelim_predictions:\n",
    "            if len(nbest) >= n_best_size:\n",
    "                break\n",
    "            feature = features[pred.feature_index]\n",
    "            if pred.start_index > 0:  # this is a non-null prediction\n",
    "                tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
    "                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
    "                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
    "                orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
    "                tok_text = \" \".join(tok_tokens)\n",
    "\n",
    "                # De-tokenize WordPieces that have been split off.\n",
    "                tok_text = tok_text.replace(\" ##\", \"\")\n",
    "                tok_text = tok_text.replace(\"##\", \"\")\n",
    "\n",
    "                # Clean whitespace\n",
    "                tok_text = tok_text.strip()\n",
    "                tok_text = \" \".join(tok_text.split())\n",
    "                orig_text = \" \".join(orig_tokens)\n",
    "\n",
    "                final_text = us.get_final_text(tok_text, orig_text, do_lower_case, verbose_logging)\n",
    "                if final_text in seen_predictions:\n",
    "                    continue\n",
    "\n",
    "                seen_predictions[final_text] = True\n",
    "            else:\n",
    "                final_text = \"\"\n",
    "                seen_predictions[final_text] = True\n",
    "\n",
    "            nbest.append(\n",
    "                _NbestPrediction(\n",
    "                    text=final_text,\n",
    "                    start_logit=pred.start_logit,\n",
    "                    end_logit=pred.end_logit))\n",
    "        # if we didn't include the empty option in the n-best, include it\n",
    "        if version_2_with_negative:\n",
    "            if \"\" not in seen_predictions:\n",
    "                nbest.append(\n",
    "                    _NbestPrediction(\n",
    "                        text=\"\",\n",
    "                        start_logit=null_start_logit,\n",
    "                        end_logit=null_end_logit))\n",
    "                \n",
    "            # In very rare edge cases we could only have single null prediction.\n",
    "            # So we just create a nonce prediction in this case to avoid failure.\n",
    "            if len(nbest)==1:\n",
    "                nbest.insert(0,\n",
    "                    _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
    "\n",
    "        # In very rare edge cases we could have no valid predictions. So we\n",
    "        # just create a nonce prediction in this case to avoid failure.\n",
    "        if not nbest:\n",
    "            nbest.append(\n",
    "                _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
    "\n",
    "        assert len(nbest) >= 1\n",
    "\n",
    "        total_scores = []\n",
    "        best_non_null_entry = None\n",
    "        for entry in nbest:\n",
    "            total_scores.append(entry.start_logit + entry.end_logit)\n",
    "            if not best_non_null_entry:\n",
    "                if entry.text:\n",
    "                    best_non_null_entry = entry\n",
    "\n",
    "        probs = us._compute_softmax(total_scores)\n",
    "\n",
    "        nbest_json = []\n",
    "        for (i, entry) in enumerate(nbest):\n",
    "            output = collections.OrderedDict()\n",
    "            output[\"text\"] = entry.text\n",
    "            output[\"probability\"] = probs[i]\n",
    "            output[\"start_logit\"] = entry.start_logit\n",
    "            output[\"end_logit\"] = entry.end_logit\n",
    "            nbest_json.append(output)\n",
    "\n",
    "        assert len(nbest_json) >= 1\n",
    "\n",
    "        if not version_2_with_negative:\n",
    "            all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n",
    "        else:\n",
    "            # predict \"\" iff the null score - the score of best non-null > threshold\n",
    "            score_diff = score_null - best_non_null_entry.start_logit - (\n",
    "                best_non_null_entry.end_logit)\n",
    "            scores_diff_json[example.qas_id] = score_diff\n",
    "            if score_diff > null_score_diff_threshold:\n",
    "                all_predictions[example.qas_id] = \"\"\n",
    "            else:\n",
    "                all_predictions[example.qas_id] = best_non_null_entry.text\n",
    "        all_nbest_json[example.qas_id] = nbest_json\n",
    "\n",
    "#     with open(output_prediction_file, \"w\") as writer:\n",
    "#         writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
    "\n",
    "#     with open(output_nbest_file, \"w\") as writer:\n",
    "#         writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
    "\n",
    "#     if version_2_with_negative:\n",
    "#         with open(output_null_log_odds_file, \"w\") as writer:\n",
    "#             writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n",
    "\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL-test.ipynb\t  rl_state_dict.pt\ttest_examples.py\r\n",
      "RL_server.ipynb   run_bertology.py\ttests_samples\r\n",
      "__pycache__\t  run_generation.py\tutils_glue.py\r\n",
      "lm_finetuning\t  run_glue.py\t\tutils_squad.py\r\n",
      "outputs\t\t  run_squad.py\t\tutils_squad_evaluate.py\r\n",
      "requirements.txt  single_model_scripts\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "used_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(all_predictions.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(batch[0][0].flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_required_examples(used_features, all_examples):\n",
    "    used_examples = []\n",
    "    for f in used_features:\n",
    "        used_examples.append(all_examples[f.example_index])\n",
    "    return used_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# os.environ['RANK'] = '0'\n",
    "print(os.environ['CUDA_VISIBLE_DEVICES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=-1 not in [-1, 0])\n",
    "for step, batch in enumerate(epoch_iterator):\n",
    "    batch = tuple(t.to(args.device) for t in batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**form_inputs(batch))\n",
    "loss = outputs[0]\n",
    "            \n",
    "results, used_features = get_all_results(batch, outputs)\n",
    "used_examples = get_required_examples(used_features, examples)\n",
    "all_predictions = write_predictions(used_examples, used_features, results, args.n_best_size,\n",
    "                        args.max_answer_length, args.do_lower_case, output_prediction_file,\n",
    "                        output_nbest_file, output_null_log_odds_file, args.verbose_logging,\n",
    "                        args.version_2_with_negative, args.null_score_diff_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('56d45abf2ccc5a1400d830e9', '&B'),\n",
       " ('56bf79c73aeaaa14008c966f', 'talent show circuit')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(all_predictions.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rewards(all_predictions, used_examples):\n",
    "    ## Convert all_predictions into list\n",
    "    pred_list = list(all_predictions.items()) ## List of tuples\n",
    "    ans_list = []\n",
    "    bleu_loss = []\n",
    "    rougel_f_scores = []\n",
    "    rouge = Rouge()\n",
    "    for id_x, ans in pred_list:\n",
    "        ans_list.append(word_tokenize(ans)) ## List of all the answers. shape - (batch_size,)\n",
    "    \n",
    "    actual_answers = []\n",
    "    \n",
    "    for ex in used_examples:\n",
    "        st = ex.start_position\n",
    "        end = ex.end_position\n",
    "        actual_ans = ex.doc_tokens[st:end]\n",
    "        actual_answers.append(actual_ans) ## List of actual answers from train set\n",
    "   \n",
    "    for i in range(len(ans_list)):\n",
    "        bleu_loss.append(1 - nltk.translate.bleu_score.sentence_bleu([actual_answers[i]], ans_list[i]))\n",
    "        try:\n",
    "            scores = rouge.get_scores(\" \".join(ans_list[i]), \" \".join(actual_answers[i]))\n",
    "            rougel_f_scores.append(1 - scores[0]['rouge-l']['f'])\n",
    "        except:\n",
    "            rougel_f_scores.append(0.0)\n",
    "#         print(scores)\n",
    "        \n",
    "    bleu_avg = sum(bleu_loss)/len(bleu_loss)\n",
    "    rouge_avg = sum(rougel_f_scores)/len(rougel_f_scores)\n",
    "    \n",
    "    total_loss = bleu_avg + rouge_avg\n",
    "        \n",
    "    return total_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "bls = calc_rewards(all_predictions, used_examples)\n",
    "print(bls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = outputs[1][0]\n",
    "start_logits_sq = start_logits.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = used_examples[0].doc_tokens\n",
    "\" \".join(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_examples[1].doc_tokens[used_examples[1].start_position : used_examples[1].end_position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_venv",
   "language": "python",
   "name": "rl_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
